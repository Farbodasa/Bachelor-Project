{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (5432, 113)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('junit.csv')\n",
    "data['Number of bugs'] = data['Number of bugs'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(\"Data size: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>LongName</th>\n",
       "      <th>Parent</th>\n",
       "      <th>Component</th>\n",
       "      <th>Path</th>\n",
       "      <th>Line</th>\n",
       "      <th>Column</th>\n",
       "      <th>EndLine</th>\n",
       "      <th>EndColumn</th>\n",
       "      <th>CC</th>\n",
       "      <th>CCL</th>\n",
       "      <th>CCO</th>\n",
       "      <th>CI</th>\n",
       "      <th>CLC</th>\n",
       "      <th>CLLC</th>\n",
       "      <th>LDC</th>\n",
       "      <th>LLDC</th>\n",
       "      <th>LCOM5</th>\n",
       "      <th>NL</th>\n",
       "      <th>NLE</th>\n",
       "      <th>WMC</th>\n",
       "      <th>CBO</th>\n",
       "      <th>CBOI</th>\n",
       "      <th>NII</th>\n",
       "      <th>NOI</th>\n",
       "      <th>RFC</th>\n",
       "      <th>AD</th>\n",
       "      <th>CD</th>\n",
       "      <th>CLOC</th>\n",
       "      <th>DLOC</th>\n",
       "      <th>PDA</th>\n",
       "      <th>PUA</th>\n",
       "      <th>TCD</th>\n",
       "      <th>TCLOC</th>\n",
       "      <th>DIT</th>\n",
       "      <th>NOA</th>\n",
       "      <th>NOC</th>\n",
       "      <th>NOD</th>\n",
       "      <th>NOP</th>\n",
       "      <th>LLOC</th>\n",
       "      <th>LOC</th>\n",
       "      <th>NA</th>\n",
       "      <th>NG</th>\n",
       "      <th>NLA</th>\n",
       "      <th>NLG</th>\n",
       "      <th>NLM</th>\n",
       "      <th>NLPA</th>\n",
       "      <th>NLPM</th>\n",
       "      <th>NLS</th>\n",
       "      <th>NM</th>\n",
       "      <th>NOS</th>\n",
       "      <th>NPA</th>\n",
       "      <th>NPM</th>\n",
       "      <th>NS</th>\n",
       "      <th>TLLOC</th>\n",
       "      <th>TLOC</th>\n",
       "      <th>TNA</th>\n",
       "      <th>TNG</th>\n",
       "      <th>TNLA</th>\n",
       "      <th>TNLG</th>\n",
       "      <th>TNLM</th>\n",
       "      <th>TNLPA</th>\n",
       "      <th>TNLPM</th>\n",
       "      <th>TNLS</th>\n",
       "      <th>TNM</th>\n",
       "      <th>TNOS</th>\n",
       "      <th>TNPA</th>\n",
       "      <th>TNPM</th>\n",
       "      <th>TNS</th>\n",
       "      <th>WarningBlocker</th>\n",
       "      <th>WarningCritical</th>\n",
       "      <th>WarningInfo</th>\n",
       "      <th>WarningMajor</th>\n",
       "      <th>WarningMinor</th>\n",
       "      <th>Android Rules</th>\n",
       "      <th>Basic Rules</th>\n",
       "      <th>Brace Rules</th>\n",
       "      <th>Clone Implementation Rules</th>\n",
       "      <th>Clone Metric Rules</th>\n",
       "      <th>Code Size Rules</th>\n",
       "      <th>Cohesion Metric Rules</th>\n",
       "      <th>Comment Rules</th>\n",
       "      <th>Complexity Metric Rules</th>\n",
       "      <th>Controversial Rules</th>\n",
       "      <th>Coupling Metric Rules</th>\n",
       "      <th>Coupling Rules</th>\n",
       "      <th>Design Rules</th>\n",
       "      <th>Documentation Metric Rules</th>\n",
       "      <th>Empty Code Rules</th>\n",
       "      <th>Finalizer Rules</th>\n",
       "      <th>Import Statement Rules</th>\n",
       "      <th>Inheritance Metric Rules</th>\n",
       "      <th>J2EE Rules</th>\n",
       "      <th>JUnit Rules</th>\n",
       "      <th>Jakarta Commons Logging Rules</th>\n",
       "      <th>Java Logging Rules</th>\n",
       "      <th>JavaBean Rules</th>\n",
       "      <th>MigratingToJUnit4 Rules</th>\n",
       "      <th>Migration Rules</th>\n",
       "      <th>Migration13 Rules</th>\n",
       "      <th>Migration14 Rules</th>\n",
       "      <th>Migration15 Rules</th>\n",
       "      <th>Naming Rules</th>\n",
       "      <th>Optimization Rules</th>\n",
       "      <th>Security Code Guideline Rules</th>\n",
       "      <th>Size Metric Rules</th>\n",
       "      <th>Strict Exception Rules</th>\n",
       "      <th>String and StringBuffer Rules</th>\n",
       "      <th>Type Resolution Rules</th>\n",
       "      <th>Unnecessary and Unused Code Rules</th>\n",
       "      <th>Vulnerability Rules</th>\n",
       "      <th>Number of bugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L49123</td>\n",
       "      <td>Axis</td>\n",
       "      <td>plotter.Axis</td>\n",
       "      <td>L48745</td>\n",
       "      <td>L103</td>\n",
       "      <td>Plotter\\src\\main\\java\\plotter\\Axis.java</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L49203</td>\n",
       "      <td>AxisLabel</td>\n",
       "      <td>plotter.AxisLabel</td>\n",
       "      <td>L48745</td>\n",
       "      <td>L103</td>\n",
       "      <td>Plotter\\src\\main\\java\\plotter\\AxisLabel.java</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L24892</td>\n",
       "      <td>DateNumberFormat</td>\n",
       "      <td>plotter.DateNumberFormat</td>\n",
       "      <td>L48745</td>\n",
       "      <td>L103</td>\n",
       "      <td>Plotter\\src\\main\\java\\plotter\\DateNumberFormat...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L22934</td>\n",
       "      <td>DoubleData</td>\n",
       "      <td>plotter.DoubleData</td>\n",
       "      <td>L48745</td>\n",
       "      <td>L103</td>\n",
       "      <td>Plotter\\src\\main\\java\\plotter\\DoubleData.java</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>522</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072788</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.064309</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.308889</td>\n",
       "      <td>139</td>\n",
       "      <td>127</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.308889</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>495</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>495</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L48935</td>\n",
       "      <td>ExpFormat</td>\n",
       "      <td>plotter.ExpFormat</td>\n",
       "      <td>L48745</td>\n",
       "      <td>L103</td>\n",
       "      <td>Plotter\\src\\main\\java\\plotter\\ExpFormat.java</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID              Name                  LongName  Parent Component  \\\n",
       "0  L49123              Axis              plotter.Axis  L48745      L103   \n",
       "1  L49203         AxisLabel         plotter.AxisLabel  L48745      L103   \n",
       "2  L24892  DateNumberFormat  plotter.DateNumberFormat  L48745      L103   \n",
       "3  L22934        DoubleData        plotter.DoubleData  L48745      L103   \n",
       "4  L48935         ExpFormat         plotter.ExpFormat  L48745      L103   \n",
       "\n",
       "                                                Path  Line  Column  EndLine  \\\n",
       "0            Plotter\\src\\main\\java\\plotter\\Axis.java    33       1      183   \n",
       "1       Plotter\\src\\main\\java\\plotter\\AxisLabel.java    30       1       55   \n",
       "2  Plotter\\src\\main\\java\\plotter\\DateNumberFormat...    34       1       76   \n",
       "3      Plotter\\src\\main\\java\\plotter\\DoubleData.java    28       1      522   \n",
       "4       Plotter\\src\\main\\java\\plotter\\ExpFormat.java    34       1       75   \n",
       "\n",
       "   EndColumn        CC  CCL  CCO  CI       CLC      CLLC  LDC  LLDC  LCOM5  \\\n",
       "0          2  0.000000    0    0   0  0.000000  0.000000    0     0      2   \n",
       "1          2  0.000000    0    0   0  0.000000  0.000000    0     0      0   \n",
       "2          2  0.000000    0    0   0  0.000000  0.000000    0     0      1   \n",
       "3          2  0.072788    1    8   2  0.042424  0.064309   21    20      1   \n",
       "4          2  0.000000    0    0   0  0.000000  0.000000    0     0      1   \n",
       "\n",
       "   NL  NLE  WMC  CBO  CBOI  NII  NOI  RFC        AD        CD  CLOC  DLOC  \\\n",
       "0   1    1   16    0    65  111    0   13  1.000000  0.515873    65    65   \n",
       "1   0    0    2    0     1    1    0    2  1.000000  0.560000    14    14   \n",
       "2   0    0    5    0     6    7    0    5  0.500000  0.333333    12    12   \n",
       "3   3    3   98    0    23  119    0   22  0.956522  0.308889   139   127   \n",
       "4   0    0    5    0     2    3    0    5  0.500000  0.405405    15    15   \n",
       "\n",
       "   PDA  PUA       TCD  TCLOC  DIT  NOA  NOC  NOD  NOP  LLOC  LOC  NA  NG  NLA  \\\n",
       "0   14    0  0.515873     65    0    0    5    8    0    61  151   7   5    7   \n",
       "1    3    0  0.560000     14    0    0    0    0    0    11   26   2   1    2   \n",
       "2    3    3  0.333333     12    0    0    0    0    0    24   43   2   1    2   \n",
       "3   22    1  0.308889    139    0    0    0    0    0   311  495   4   2    4   \n",
       "4    3    3  0.405405     15    0    0    0    0    0    22   42   2   1    2   \n",
       "\n",
       "   NLG  NLM  NLPA  NLPM  NLS  NM  NOS  NPA  NPM  NS  TLLOC  TLOC  TNA  TNG  \\\n",
       "0    5   13     0    13    6  13   23    0   13   6     61   151    7    5   \n",
       "1    1    2     0     2    0   2    3    0    2   0     11    26    2    1   \n",
       "2    1    5     0     5    0   5    7    0    5   0     24    43    2    1   \n",
       "3    2   22     0    22    1  22  190    0   22   1    311   495    4    2   \n",
       "4    1    5     0     5    0   5    5    0    5   0     22    42    2    1   \n",
       "\n",
       "   TNLA  TNLG  TNLM  TNLPA  TNLPM  TNLS  TNM  TNOS  TNPA  TNPM  TNS  \\\n",
       "0     7     5    13      0     13     6   13    23     0    13    6   \n",
       "1     2     1     2      0      2     0    2     3     0     2    0   \n",
       "2     2     1     5      0      5     0    5     7     0     5    0   \n",
       "3     4     2    22      0     22     1   22   190     0    22    1   \n",
       "4     2     1     5      0      5     0    5     5     0     5    0   \n",
       "\n",
       "   WarningBlocker  WarningCritical  WarningInfo  WarningMajor  WarningMinor  \\\n",
       "0               0                0            3             0             1   \n",
       "1               0                0            0             0             1   \n",
       "2               0                0            7             0             1   \n",
       "3               0                0           25             6             2   \n",
       "4               0                0            7             0             1   \n",
       "\n",
       "   Android Rules  Basic Rules  Brace Rules  Clone Implementation Rules  \\\n",
       "0              0            0            0                           0   \n",
       "1              0            0            0                           0   \n",
       "2              0            0            0                           0   \n",
       "3              0            0            0                           1   \n",
       "4              0            0            0                           0   \n",
       "\n",
       "   Clone Metric Rules  Code Size Rules  Cohesion Metric Rules  Comment Rules  \\\n",
       "0                   0                0                      1              0   \n",
       "1                   0                0                      0              0   \n",
       "2                   0                0                      0              0   \n",
       "3                  14                0                      0              0   \n",
       "4                   0                0                      0              0   \n",
       "\n",
       "   Complexity Metric Rules  Controversial Rules  Coupling Metric Rules  \\\n",
       "0                        0                    0                      0   \n",
       "1                        0                    0                      0   \n",
       "2                        0                    0                      0   \n",
       "3                        0                    0                      0   \n",
       "4                        0                    0                      0   \n",
       "\n",
       "   Coupling Rules  Design Rules  Documentation Metric Rules  Empty Code Rules  \\\n",
       "0               0             0                           0                 0   \n",
       "1               0             1                           0                 0   \n",
       "2               0             0                           7                 0   \n",
       "3               0             2                           9                 0   \n",
       "4               0             1                           7                 0   \n",
       "\n",
       "   Finalizer Rules  Import Statement Rules  Inheritance Metric Rules  \\\n",
       "0                0                       0                         0   \n",
       "1                0                       0                         0   \n",
       "2                0                       0                         0   \n",
       "3                0                       0                         0   \n",
       "4                0                       0                         0   \n",
       "\n",
       "   J2EE Rules  JUnit Rules  Jakarta Commons Logging Rules  Java Logging Rules  \\\n",
       "0           0            0                              0                   0   \n",
       "1           0            0                              0                   0   \n",
       "2           0            0                              0                   0   \n",
       "3           0            0                              0                   0   \n",
       "4           0            0                              0                   0   \n",
       "\n",
       "   JavaBean Rules  MigratingToJUnit4 Rules  Migration Rules  \\\n",
       "0               0                        0                0   \n",
       "1               0                        0                0   \n",
       "2               0                        0                0   \n",
       "3               0                        0                0   \n",
       "4               0                        0                0   \n",
       "\n",
       "   Migration13 Rules  Migration14 Rules  Migration15 Rules  Naming Rules  \\\n",
       "0                  0                  0                  0             1   \n",
       "1                  0                  0                  0             0   \n",
       "2                  0                  0                  0             1   \n",
       "3                  0                  0                  0             0   \n",
       "4                  0                  0                  0             0   \n",
       "\n",
       "   Optimization Rules  Security Code Guideline Rules  Size Metric Rules  \\\n",
       "0                   0                              0                  2   \n",
       "1                   0                              0                  0   \n",
       "2                   0                              0                  0   \n",
       "3                   0                              0                  2   \n",
       "4                   0                              0                  0   \n",
       "\n",
       "   Strict Exception Rules  String and StringBuffer Rules  \\\n",
       "0                       0                              0   \n",
       "1                       0                              0   \n",
       "2                       0                              0   \n",
       "3                       1                              4   \n",
       "4                       0                              0   \n",
       "\n",
       "   Type Resolution Rules  Unnecessary and Unused Code Rules  \\\n",
       "0                      0                                  0   \n",
       "1                      0                                  0   \n",
       "2                      0                                  0   \n",
       "3                      0                                  0   \n",
       "4                      0                                  0   \n",
       "\n",
       "   Vulnerability Rules  Number of bugs  \n",
       "0                    0               0  \n",
       "1                    0               0  \n",
       "2                    0               0  \n",
       "3                    0               0  \n",
       "4                    0               0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Column</th>\n",
       "      <th>EndLine</th>\n",
       "      <th>EndColumn</th>\n",
       "      <th>CC</th>\n",
       "      <th>CCL</th>\n",
       "      <th>CCO</th>\n",
       "      <th>CI</th>\n",
       "      <th>CLC</th>\n",
       "      <th>CLLC</th>\n",
       "      <th>LDC</th>\n",
       "      <th>LLDC</th>\n",
       "      <th>LCOM5</th>\n",
       "      <th>NL</th>\n",
       "      <th>NLE</th>\n",
       "      <th>WMC</th>\n",
       "      <th>CBO</th>\n",
       "      <th>CBOI</th>\n",
       "      <th>NII</th>\n",
       "      <th>NOI</th>\n",
       "      <th>RFC</th>\n",
       "      <th>AD</th>\n",
       "      <th>CD</th>\n",
       "      <th>CLOC</th>\n",
       "      <th>DLOC</th>\n",
       "      <th>PDA</th>\n",
       "      <th>PUA</th>\n",
       "      <th>TCD</th>\n",
       "      <th>TCLOC</th>\n",
       "      <th>DIT</th>\n",
       "      <th>NOA</th>\n",
       "      <th>NOC</th>\n",
       "      <th>NOD</th>\n",
       "      <th>NOP</th>\n",
       "      <th>LLOC</th>\n",
       "      <th>LOC</th>\n",
       "      <th>NA</th>\n",
       "      <th>NG</th>\n",
       "      <th>NLA</th>\n",
       "      <th>NLG</th>\n",
       "      <th>NLM</th>\n",
       "      <th>NLPA</th>\n",
       "      <th>NLPM</th>\n",
       "      <th>NLS</th>\n",
       "      <th>NM</th>\n",
       "      <th>NOS</th>\n",
       "      <th>NPA</th>\n",
       "      <th>NPM</th>\n",
       "      <th>NS</th>\n",
       "      <th>TLLOC</th>\n",
       "      <th>TLOC</th>\n",
       "      <th>TNA</th>\n",
       "      <th>TNG</th>\n",
       "      <th>TNLA</th>\n",
       "      <th>TNLG</th>\n",
       "      <th>TNLM</th>\n",
       "      <th>TNLPA</th>\n",
       "      <th>TNLPM</th>\n",
       "      <th>TNLS</th>\n",
       "      <th>TNM</th>\n",
       "      <th>TNOS</th>\n",
       "      <th>TNPA</th>\n",
       "      <th>TNPM</th>\n",
       "      <th>TNS</th>\n",
       "      <th>WarningBlocker</th>\n",
       "      <th>WarningCritical</th>\n",
       "      <th>WarningInfo</th>\n",
       "      <th>WarningMajor</th>\n",
       "      <th>WarningMinor</th>\n",
       "      <th>Android Rules</th>\n",
       "      <th>Basic Rules</th>\n",
       "      <th>Brace Rules</th>\n",
       "      <th>Clone Implementation Rules</th>\n",
       "      <th>Clone Metric Rules</th>\n",
       "      <th>Code Size Rules</th>\n",
       "      <th>Cohesion Metric Rules</th>\n",
       "      <th>Comment Rules</th>\n",
       "      <th>Complexity Metric Rules</th>\n",
       "      <th>Controversial Rules</th>\n",
       "      <th>Coupling Metric Rules</th>\n",
       "      <th>Coupling Rules</th>\n",
       "      <th>Design Rules</th>\n",
       "      <th>Documentation Metric Rules</th>\n",
       "      <th>Empty Code Rules</th>\n",
       "      <th>Finalizer Rules</th>\n",
       "      <th>Import Statement Rules</th>\n",
       "      <th>Inheritance Metric Rules</th>\n",
       "      <th>J2EE Rules</th>\n",
       "      <th>JUnit Rules</th>\n",
       "      <th>Jakarta Commons Logging Rules</th>\n",
       "      <th>Java Logging Rules</th>\n",
       "      <th>JavaBean Rules</th>\n",
       "      <th>MigratingToJUnit4 Rules</th>\n",
       "      <th>Migration Rules</th>\n",
       "      <th>Migration13 Rules</th>\n",
       "      <th>Migration14 Rules</th>\n",
       "      <th>Migration15 Rules</th>\n",
       "      <th>Naming Rules</th>\n",
       "      <th>Optimization Rules</th>\n",
       "      <th>Security Code Guideline Rules</th>\n",
       "      <th>Size Metric Rules</th>\n",
       "      <th>Strict Exception Rules</th>\n",
       "      <th>String and StringBuffer Rules</th>\n",
       "      <th>Type Resolution Rules</th>\n",
       "      <th>Unnecessary and Unused Code Rules</th>\n",
       "      <th>Vulnerability Rules</th>\n",
       "      <th>Number of bugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>522</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072788</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.064309</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.308889</td>\n",
       "      <td>139</td>\n",
       "      <td>127</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.308889</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>495</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>495</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Line  Column  EndLine  EndColumn        CC  CCL  CCO  CI       CLC  \\\n",
       "0    33       1      183          2  0.000000    0    0   0  0.000000   \n",
       "1    30       1       55          2  0.000000    0    0   0  0.000000   \n",
       "2    34       1       76          2  0.000000    0    0   0  0.000000   \n",
       "3    28       1      522          2  0.072788    1    8   2  0.042424   \n",
       "4    34       1       75          2  0.000000    0    0   0  0.000000   \n",
       "\n",
       "       CLLC  LDC  LLDC  LCOM5  NL  NLE  WMC  CBO  CBOI  NII  NOI  RFC  \\\n",
       "0  0.000000    0     0      2   1    1   16    0    65  111    0   13   \n",
       "1  0.000000    0     0      0   0    0    2    0     1    1    0    2   \n",
       "2  0.000000    0     0      1   0    0    5    0     6    7    0    5   \n",
       "3  0.064309   21    20      1   3    3   98    0    23  119    0   22   \n",
       "4  0.000000    0     0      1   0    0    5    0     2    3    0    5   \n",
       "\n",
       "         AD        CD  CLOC  DLOC  PDA  PUA       TCD  TCLOC  DIT  NOA  NOC  \\\n",
       "0  1.000000  0.515873    65    65   14    0  0.515873     65    0    0    5   \n",
       "1  1.000000  0.560000    14    14    3    0  0.560000     14    0    0    0   \n",
       "2  0.500000  0.333333    12    12    3    3  0.333333     12    0    0    0   \n",
       "3  0.956522  0.308889   139   127   22    1  0.308889    139    0    0    0   \n",
       "4  0.500000  0.405405    15    15    3    3  0.405405     15    0    0    0   \n",
       "\n",
       "   NOD  NOP  LLOC  LOC  NA  NG  NLA  NLG  NLM  NLPA  NLPM  NLS  NM  NOS  NPA  \\\n",
       "0    8    0    61  151   7   5    7    5   13     0    13    6  13   23    0   \n",
       "1    0    0    11   26   2   1    2    1    2     0     2    0   2    3    0   \n",
       "2    0    0    24   43   2   1    2    1    5     0     5    0   5    7    0   \n",
       "3    0    0   311  495   4   2    4    2   22     0    22    1  22  190    0   \n",
       "4    0    0    22   42   2   1    2    1    5     0     5    0   5    5    0   \n",
       "\n",
       "   NPM  NS  TLLOC  TLOC  TNA  TNG  TNLA  TNLG  TNLM  TNLPA  TNLPM  TNLS  TNM  \\\n",
       "0   13   6     61   151    7    5     7     5    13      0     13     6   13   \n",
       "1    2   0     11    26    2    1     2     1     2      0      2     0    2   \n",
       "2    5   0     24    43    2    1     2     1     5      0      5     0    5   \n",
       "3   22   1    311   495    4    2     4     2    22      0     22     1   22   \n",
       "4    5   0     22    42    2    1     2     1     5      0      5     0    5   \n",
       "\n",
       "   TNOS  TNPA  TNPM  TNS  WarningBlocker  WarningCritical  WarningInfo  \\\n",
       "0    23     0    13    6               0                0            3   \n",
       "1     3     0     2    0               0                0            0   \n",
       "2     7     0     5    0               0                0            7   \n",
       "3   190     0    22    1               0                0           25   \n",
       "4     5     0     5    0               0                0            7   \n",
       "\n",
       "   WarningMajor  WarningMinor  Android Rules  Basic Rules  Brace Rules  \\\n",
       "0             0             1              0            0            0   \n",
       "1             0             1              0            0            0   \n",
       "2             0             1              0            0            0   \n",
       "3             6             2              0            0            0   \n",
       "4             0             1              0            0            0   \n",
       "\n",
       "   Clone Implementation Rules  Clone Metric Rules  Code Size Rules  \\\n",
       "0                           0                   0                0   \n",
       "1                           0                   0                0   \n",
       "2                           0                   0                0   \n",
       "3                           1                  14                0   \n",
       "4                           0                   0                0   \n",
       "\n",
       "   Cohesion Metric Rules  Comment Rules  Complexity Metric Rules  \\\n",
       "0                      1              0                        0   \n",
       "1                      0              0                        0   \n",
       "2                      0              0                        0   \n",
       "3                      0              0                        0   \n",
       "4                      0              0                        0   \n",
       "\n",
       "   Controversial Rules  Coupling Metric Rules  Coupling Rules  Design Rules  \\\n",
       "0                    0                      0               0             0   \n",
       "1                    0                      0               0             1   \n",
       "2                    0                      0               0             0   \n",
       "3                    0                      0               0             2   \n",
       "4                    0                      0               0             1   \n",
       "\n",
       "   Documentation Metric Rules  Empty Code Rules  Finalizer Rules  \\\n",
       "0                           0                 0                0   \n",
       "1                           0                 0                0   \n",
       "2                           7                 0                0   \n",
       "3                           9                 0                0   \n",
       "4                           7                 0                0   \n",
       "\n",
       "   Import Statement Rules  Inheritance Metric Rules  J2EE Rules  JUnit Rules  \\\n",
       "0                       0                         0           0            0   \n",
       "1                       0                         0           0            0   \n",
       "2                       0                         0           0            0   \n",
       "3                       0                         0           0            0   \n",
       "4                       0                         0           0            0   \n",
       "\n",
       "   Jakarta Commons Logging Rules  Java Logging Rules  JavaBean Rules  \\\n",
       "0                              0                   0               0   \n",
       "1                              0                   0               0   \n",
       "2                              0                   0               0   \n",
       "3                              0                   0               0   \n",
       "4                              0                   0               0   \n",
       "\n",
       "   MigratingToJUnit4 Rules  Migration Rules  Migration13 Rules  \\\n",
       "0                        0                0                  0   \n",
       "1                        0                0                  0   \n",
       "2                        0                0                  0   \n",
       "3                        0                0                  0   \n",
       "4                        0                0                  0   \n",
       "\n",
       "   Migration14 Rules  Migration15 Rules  Naming Rules  Optimization Rules  \\\n",
       "0                  0                  0             1                   0   \n",
       "1                  0                  0             0                   0   \n",
       "2                  0                  0             1                   0   \n",
       "3                  0                  0             0                   0   \n",
       "4                  0                  0             0                   0   \n",
       "\n",
       "   Security Code Guideline Rules  Size Metric Rules  Strict Exception Rules  \\\n",
       "0                              0                  2                       0   \n",
       "1                              0                  0                       0   \n",
       "2                              0                  0                       0   \n",
       "3                              0                  2                       1   \n",
       "4                              0                  0                       0   \n",
       "\n",
       "   String and StringBuffer Rules  Type Resolution Rules  \\\n",
       "0                              0                      0   \n",
       "1                              0                      0   \n",
       "2                              0                      0   \n",
       "3                              4                      0   \n",
       "4                              0                      0   \n",
       "\n",
       "   Unnecessary and Unused Code Rules  Vulnerability Rules  Number of bugs  \n",
       "0                                  0                    0               0  \n",
       "1                                  0                    0               0  \n",
       "2                                  0                    0               0  \n",
       "3                                  0                    0               0  \n",
       "4                                  0                    0               0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns = ['ID', 'Name', 'LongName', 'Parent', 'Component', 'Path'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6091 entries, 0 to 6090\n",
      "Columns: 107 entries, Line to Number of bugs\n",
      "dtypes: float64(6), int64(101)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Column</th>\n",
       "      <th>EndLine</th>\n",
       "      <th>EndColumn</th>\n",
       "      <th>CC</th>\n",
       "      <th>CCL</th>\n",
       "      <th>CCO</th>\n",
       "      <th>CI</th>\n",
       "      <th>CLC</th>\n",
       "      <th>CLLC</th>\n",
       "      <th>LDC</th>\n",
       "      <th>LLDC</th>\n",
       "      <th>LCOM5</th>\n",
       "      <th>NL</th>\n",
       "      <th>NLE</th>\n",
       "      <th>WMC</th>\n",
       "      <th>CBO</th>\n",
       "      <th>CBOI</th>\n",
       "      <th>NII</th>\n",
       "      <th>NOI</th>\n",
       "      <th>RFC</th>\n",
       "      <th>AD</th>\n",
       "      <th>CD</th>\n",
       "      <th>CLOC</th>\n",
       "      <th>DLOC</th>\n",
       "      <th>PDA</th>\n",
       "      <th>PUA</th>\n",
       "      <th>TCD</th>\n",
       "      <th>TCLOC</th>\n",
       "      <th>DIT</th>\n",
       "      <th>NOA</th>\n",
       "      <th>NOC</th>\n",
       "      <th>NOD</th>\n",
       "      <th>NOP</th>\n",
       "      <th>LLOC</th>\n",
       "      <th>LOC</th>\n",
       "      <th>NA</th>\n",
       "      <th>NG</th>\n",
       "      <th>NLA</th>\n",
       "      <th>NLG</th>\n",
       "      <th>NLM</th>\n",
       "      <th>NLPA</th>\n",
       "      <th>NLPM</th>\n",
       "      <th>NLS</th>\n",
       "      <th>NM</th>\n",
       "      <th>NOS</th>\n",
       "      <th>NPA</th>\n",
       "      <th>NPM</th>\n",
       "      <th>NS</th>\n",
       "      <th>TLLOC</th>\n",
       "      <th>TLOC</th>\n",
       "      <th>TNA</th>\n",
       "      <th>TNG</th>\n",
       "      <th>TNLA</th>\n",
       "      <th>TNLG</th>\n",
       "      <th>TNLM</th>\n",
       "      <th>TNLPA</th>\n",
       "      <th>TNLPM</th>\n",
       "      <th>TNLS</th>\n",
       "      <th>TNM</th>\n",
       "      <th>TNOS</th>\n",
       "      <th>TNPA</th>\n",
       "      <th>TNPM</th>\n",
       "      <th>TNS</th>\n",
       "      <th>WarningBlocker</th>\n",
       "      <th>WarningCritical</th>\n",
       "      <th>WarningInfo</th>\n",
       "      <th>WarningMajor</th>\n",
       "      <th>WarningMinor</th>\n",
       "      <th>Android Rules</th>\n",
       "      <th>Basic Rules</th>\n",
       "      <th>Brace Rules</th>\n",
       "      <th>Clone Implementation Rules</th>\n",
       "      <th>Clone Metric Rules</th>\n",
       "      <th>Code Size Rules</th>\n",
       "      <th>Cohesion Metric Rules</th>\n",
       "      <th>Comment Rules</th>\n",
       "      <th>Complexity Metric Rules</th>\n",
       "      <th>Controversial Rules</th>\n",
       "      <th>Coupling Metric Rules</th>\n",
       "      <th>Coupling Rules</th>\n",
       "      <th>Design Rules</th>\n",
       "      <th>Documentation Metric Rules</th>\n",
       "      <th>Empty Code Rules</th>\n",
       "      <th>Finalizer Rules</th>\n",
       "      <th>Import Statement Rules</th>\n",
       "      <th>Inheritance Metric Rules</th>\n",
       "      <th>J2EE Rules</th>\n",
       "      <th>JUnit Rules</th>\n",
       "      <th>Jakarta Commons Logging Rules</th>\n",
       "      <th>Java Logging Rules</th>\n",
       "      <th>JavaBean Rules</th>\n",
       "      <th>MigratingToJUnit4 Rules</th>\n",
       "      <th>Migration Rules</th>\n",
       "      <th>Migration13 Rules</th>\n",
       "      <th>Migration14 Rules</th>\n",
       "      <th>Migration15 Rules</th>\n",
       "      <th>Naming Rules</th>\n",
       "      <th>Optimization Rules</th>\n",
       "      <th>Security Code Guideline Rules</th>\n",
       "      <th>Size Metric Rules</th>\n",
       "      <th>Strict Exception Rules</th>\n",
       "      <th>String and StringBuffer Rules</th>\n",
       "      <th>Type Resolution Rules</th>\n",
       "      <th>Unnecessary and Unused Code Rules</th>\n",
       "      <th>Vulnerability Rules</th>\n",
       "      <th>Number of bugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6091.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>182.390084</td>\n",
       "      <td>22.789526</td>\n",
       "      <td>260.898210</td>\n",
       "      <td>4.974224</td>\n",
       "      <td>0.192595</td>\n",
       "      <td>0.861271</td>\n",
       "      <td>3.091775</td>\n",
       "      <td>1.401904</td>\n",
       "      <td>0.180920</td>\n",
       "      <td>0.187167</td>\n",
       "      <td>14.199803</td>\n",
       "      <td>12.146446</td>\n",
       "      <td>1.369890</td>\n",
       "      <td>0.911673</td>\n",
       "      <td>0.853554</td>\n",
       "      <td>9.159416</td>\n",
       "      <td>3.823182</td>\n",
       "      <td>2.917912</td>\n",
       "      <td>3.852898</td>\n",
       "      <td>4.824167</td>\n",
       "      <td>9.888032</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.100587</td>\n",
       "      <td>9.849778</td>\n",
       "      <td>7.694796</td>\n",
       "      <td>1.231653</td>\n",
       "      <td>3.869644</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>10.736661</td>\n",
       "      <td>0.582663</td>\n",
       "      <td>0.740765</td>\n",
       "      <td>0.264817</td>\n",
       "      <td>0.353308</td>\n",
       "      <td>0.399113</td>\n",
       "      <td>49.727467</td>\n",
       "      <td>69.293055</td>\n",
       "      <td>4.994582</td>\n",
       "      <td>3.515351</td>\n",
       "      <td>2.494993</td>\n",
       "      <td>1.167953</td>\n",
       "      <td>5.063865</td>\n",
       "      <td>0.293876</td>\n",
       "      <td>4.100969</td>\n",
       "      <td>0.524380</td>\n",
       "      <td>12.630931</td>\n",
       "      <td>26.930717</td>\n",
       "      <td>0.770645</td>\n",
       "      <td>9.589558</td>\n",
       "      <td>1.324085</td>\n",
       "      <td>57.719094</td>\n",
       "      <td>79.508127</td>\n",
       "      <td>6.200788</td>\n",
       "      <td>5.550156</td>\n",
       "      <td>2.937285</td>\n",
       "      <td>1.484814</td>\n",
       "      <td>6.308324</td>\n",
       "      <td>0.456083</td>\n",
       "      <td>5.168281</td>\n",
       "      <td>0.629125</td>\n",
       "      <td>19.652602</td>\n",
       "      <td>30.089640</td>\n",
       "      <td>1.031686</td>\n",
       "      <td>14.734855</td>\n",
       "      <td>2.015104</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.232638</td>\n",
       "      <td>24.945986</td>\n",
       "      <td>0.705303</td>\n",
       "      <td>4.699721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125923</td>\n",
       "      <td>0.639796</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>7.782958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.422427</td>\n",
       "      <td>0.236086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.913315</td>\n",
       "      <td>14.002298</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>0.097849</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>1.301593</td>\n",
       "      <td>0.057133</td>\n",
       "      <td>0.067477</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.219833</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>2.265474</td>\n",
       "      <td>0.088327</td>\n",
       "      <td>0.143490</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.057626</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.003776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>315.432139</td>\n",
       "      <td>32.431548</td>\n",
       "      <td>332.921013</td>\n",
       "      <td>6.903855</td>\n",
       "      <td>0.355994</td>\n",
       "      <td>2.739535</td>\n",
       "      <td>15.888464</td>\n",
       "      <td>6.760556</td>\n",
       "      <td>0.344846</td>\n",
       "      <td>0.349758</td>\n",
       "      <td>61.008869</td>\n",
       "      <td>52.185814</td>\n",
       "      <td>2.422718</td>\n",
       "      <td>1.447861</td>\n",
       "      <td>1.253656</td>\n",
       "      <td>20.163283</td>\n",
       "      <td>5.019446</td>\n",
       "      <td>13.332830</td>\n",
       "      <td>18.059581</td>\n",
       "      <td>10.364258</td>\n",
       "      <td>17.021698</td>\n",
       "      <td>0.319793</td>\n",
       "      <td>0.171450</td>\n",
       "      <td>29.676071</td>\n",
       "      <td>26.280299</td>\n",
       "      <td>4.369249</td>\n",
       "      <td>6.663634</td>\n",
       "      <td>0.168732</td>\n",
       "      <td>31.808638</td>\n",
       "      <td>0.903844</td>\n",
       "      <td>1.354774</td>\n",
       "      <td>2.560655</td>\n",
       "      <td>4.095528</td>\n",
       "      <td>0.546485</td>\n",
       "      <td>100.065592</td>\n",
       "      <td>138.553520</td>\n",
       "      <td>8.821202</td>\n",
       "      <td>7.537049</td>\n",
       "      <td>6.428878</td>\n",
       "      <td>3.497435</td>\n",
       "      <td>9.299277</td>\n",
       "      <td>3.680864</td>\n",
       "      <td>8.124452</td>\n",
       "      <td>2.710229</td>\n",
       "      <td>22.143287</td>\n",
       "      <td>67.044515</td>\n",
       "      <td>3.901551</td>\n",
       "      <td>16.409226</td>\n",
       "      <td>3.654591</td>\n",
       "      <td>119.946655</td>\n",
       "      <td>163.390293</td>\n",
       "      <td>11.424899</td>\n",
       "      <td>27.167387</td>\n",
       "      <td>8.332953</td>\n",
       "      <td>4.272902</td>\n",
       "      <td>11.762475</td>\n",
       "      <td>5.229872</td>\n",
       "      <td>10.249855</td>\n",
       "      <td>3.100379</td>\n",
       "      <td>106.686336</td>\n",
       "      <td>75.668423</td>\n",
       "      <td>5.418343</td>\n",
       "      <td>74.769494</td>\n",
       "      <td>9.618542</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>1.325966</td>\n",
       "      <td>54.168680</td>\n",
       "      <td>2.426117</td>\n",
       "      <td>15.457425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.061369</td>\n",
       "      <td>2.811532</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>27.985045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.047868</td>\n",
       "      <td>2.394304</td>\n",
       "      <td>0.928698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.344052</td>\n",
       "      <td>22.865402</td>\n",
       "      <td>0.215813</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>1.543435</td>\n",
       "      <td>1.125877</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>10.250579</td>\n",
       "      <td>0.535155</td>\n",
       "      <td>0.582907</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.243723</td>\n",
       "      <td>1.156451</td>\n",
       "      <td>0.202915</td>\n",
       "      <td>9.311114</td>\n",
       "      <td>0.562437</td>\n",
       "      <td>1.039814</td>\n",
       "      <td>0.743115</td>\n",
       "      <td>0.437333</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>0.061339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>170.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.178326</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3075.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>3510.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>603.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>629.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1617.000000</td>\n",
       "      <td>2327.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>1177.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>2529.000000</td>\n",
       "      <td>3411.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>1099.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4602.000000</td>\n",
       "      <td>1679.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Line       Column      EndLine    EndColumn           CC  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean    182.390084    22.789526   260.898210     4.974224     0.192595   \n",
       "std     315.432139    32.431548   332.921013     6.903855     0.355994   \n",
       "min       3.000000     1.000000     5.000000     2.000000     0.000000   \n",
       "25%      36.000000     1.000000    82.000000     2.000000     0.000000   \n",
       "50%      57.000000     2.000000   141.000000     3.000000     0.000000   \n",
       "75%     170.000000    49.000000   296.000000     6.000000     0.193548   \n",
       "max    3075.000000   206.000000  3510.000000    88.000000     1.000000   \n",
       "\n",
       "               CCL          CCO           CI          CLC         CLLC  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      0.861271     3.091775     1.401904     0.180920     0.187167   \n",
       "std       2.739535    15.888464     6.760556     0.344846     0.349758   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     0.148148     0.178326   \n",
       "max      50.000000   314.000000   137.000000     1.000000     1.000000   \n",
       "\n",
       "               LDC         LLDC        LCOM5           NL          NLE  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean     14.199803    12.146446     1.369890     0.911673     0.853554   \n",
       "std      61.008869    52.185814     2.422718     1.447861     1.253656   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "75%       6.500000     6.000000     1.000000     1.000000     1.000000   \n",
       "max     998.000000   869.000000    60.000000    18.000000     9.000000   \n",
       "\n",
       "               WMC          CBO         CBOI          NII          NOI  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      9.159416     3.823182     2.917912     3.852898     4.824167   \n",
       "std      20.163283     5.019446    13.332830    18.059581    10.364258   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "50%       3.000000     2.000000     1.000000     0.000000     1.000000   \n",
       "75%       8.000000     5.000000     2.000000     1.000000     5.000000   \n",
       "max     413.000000    56.000000   409.000000   503.000000   199.000000   \n",
       "\n",
       "               RFC           AD           CD         CLOC         DLOC  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      9.888032     0.163900     0.100587     9.849778     7.694796   \n",
       "std      17.021698     0.319793     0.171450    29.676071    26.280299   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       4.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%      11.000000     0.153846     0.133333     7.000000     4.000000   \n",
       "max     246.000000     1.000000     0.818182   613.000000   603.000000   \n",
       "\n",
       "               PDA          PUA          TCD        TCLOC          DIT  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      1.231653     3.869644     0.098506    10.736661     0.582663   \n",
       "std       4.369249     6.663634     0.168732    31.808638     0.903844   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     2.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     2.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     4.000000     0.129630     7.000000     1.000000   \n",
       "max     104.000000   100.000000     0.818182   629.000000     5.000000   \n",
       "\n",
       "               NOA          NOC          NOD          NOP         LLOC  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      0.740765     0.264817     0.353308     0.399113    49.727467   \n",
       "std       1.354774     2.560655     4.095528     0.546485   100.065592   \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     9.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000    18.000000   \n",
       "75%       1.000000     0.000000     0.000000     1.000000    48.000000   \n",
       "max       7.000000    65.000000   132.000000     4.000000  1617.000000   \n",
       "\n",
       "               LOC           NA           NG          NLA          NLG  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean     69.293055     4.994582     3.515351     2.494993     1.167953   \n",
       "std     138.553520     8.821202     7.537049     6.428878     3.497435   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      10.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%      25.000000     2.000000     0.000000     1.000000     0.000000   \n",
       "75%      66.000000     6.000000     2.000000     3.000000     1.000000   \n",
       "max    2327.000000   133.000000    60.000000   133.000000    45.000000   \n",
       "\n",
       "               NLM         NLPA         NLPM          NLS           NM  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      5.063865     0.293876     4.100969     0.524380    12.630931   \n",
       "std       9.299277     3.680864     8.124452     2.710229    22.143287   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000     1.000000     0.000000     1.000000   \n",
       "50%       2.000000     0.000000     2.000000     0.000000     4.000000   \n",
       "75%       5.000000     0.000000     4.000000     0.000000    10.000000   \n",
       "max     133.000000   122.000000   103.000000    51.000000   175.000000   \n",
       "\n",
       "               NOS          NPA          NPM           NS        TLLOC  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean     26.930717     0.770645     9.589558     1.324085    57.719094   \n",
       "std      67.044515     3.901551    16.409226     3.654591   119.946655   \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "25%       2.000000     0.000000     1.000000     0.000000     9.000000   \n",
       "50%       6.000000     0.000000     3.000000     0.000000    19.000000   \n",
       "75%      23.000000     0.000000     8.000000     1.000000    56.000000   \n",
       "max    1177.000000   122.000000   121.000000    51.000000  2529.000000   \n",
       "\n",
       "              TLOC          TNA          TNG         TNLA         TNLG  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean     79.508127     6.200788     5.550156     2.937285     1.484814   \n",
       "std     163.390293    11.424899    27.167387     8.332953     4.272902   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      11.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%      26.000000     2.000000     1.000000     1.000000     0.000000   \n",
       "75%      78.000000     7.000000     4.000000     3.000000     1.000000   \n",
       "max    3411.000000   183.000000  1099.000000   183.000000    50.000000   \n",
       "\n",
       "              TNLM        TNLPA        TNLPM         TNLS          TNM  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000  6091.000000   \n",
       "mean      6.308324     0.456083     5.168281     0.629125    19.652602   \n",
       "std      11.762475     5.229872    10.249855     3.100379   106.686336   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000     1.000000     0.000000     1.000000   \n",
       "50%       3.000000     0.000000     2.000000     0.000000     4.000000   \n",
       "75%       6.000000     0.000000     5.000000     0.000000    14.000000   \n",
       "max     138.000000   163.000000   120.000000    53.000000  4602.000000   \n",
       "\n",
       "              TNOS         TNPA         TNPM          TNS  WarningBlocker  \\\n",
       "count  6091.000000  6091.000000  6091.000000  6091.000000     6091.000000   \n",
       "mean     30.089640     1.031686    14.734855     2.015104        0.000493   \n",
       "std      75.668423     5.418343    74.769494     9.618542        0.022189   \n",
       "min       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
       "25%       2.000000     0.000000     1.000000     0.000000        0.000000   \n",
       "50%       7.000000     0.000000     3.000000     0.000000        0.000000   \n",
       "75%      27.000000     0.000000    11.000000     1.000000        0.000000   \n",
       "max    1679.000000   163.000000  3220.000000   409.000000        1.000000   \n",
       "\n",
       "       WarningCritical  WarningInfo  WarningMajor  WarningMinor  \\\n",
       "count      6091.000000  6091.000000   6091.000000   6091.000000   \n",
       "mean          0.232638    24.945986      0.705303      4.699721   \n",
       "std           1.325966    54.168680      2.426117     15.457425   \n",
       "min           0.000000     0.000000      0.000000      0.000000   \n",
       "25%           0.000000     5.000000      0.000000      0.000000   \n",
       "50%           0.000000    12.000000      0.000000      1.000000   \n",
       "75%           0.000000    23.000000      0.000000      3.000000   \n",
       "max          22.000000  1390.000000     37.000000    318.000000   \n",
       "\n",
       "       Android Rules  Basic Rules  Brace Rules  Clone Implementation Rules  \\\n",
       "count         6091.0  6091.000000  6091.000000                 6091.000000   \n",
       "mean             0.0     0.125923     0.639796                    0.007224   \n",
       "std              0.0     1.061369     2.811532                    0.105421   \n",
       "min              0.0     0.000000     0.000000                    0.000000   \n",
       "25%              0.0     0.000000     0.000000                    0.000000   \n",
       "50%              0.0     0.000000     0.000000                    0.000000   \n",
       "75%              0.0     0.000000     0.000000                    0.000000   \n",
       "max              0.0    22.000000    42.000000                    2.000000   \n",
       "\n",
       "       Clone Metric Rules  Code Size Rules  Cohesion Metric Rules  \\\n",
       "count         6091.000000           6091.0            6091.000000   \n",
       "mean             7.782958              0.0               0.334264   \n",
       "std             27.985045              0.0               0.674353   \n",
       "min              0.000000              0.0               0.000000   \n",
       "25%              0.000000              0.0               0.000000   \n",
       "50%              0.000000              0.0               0.000000   \n",
       "75%             10.000000              0.0               1.000000   \n",
       "max            663.000000              0.0               8.000000   \n",
       "\n",
       "       Comment Rules  Complexity Metric Rules  Controversial Rules  \\\n",
       "count         6091.0              6091.000000          6091.000000   \n",
       "mean             0.0                 0.227056             0.422427   \n",
       "std              0.0                 1.047868             2.394304   \n",
       "min              0.0                 0.000000             0.000000   \n",
       "25%              0.0                 0.000000             0.000000   \n",
       "50%              0.0                 0.000000             0.000000   \n",
       "75%              0.0                 0.000000             0.000000   \n",
       "max              0.0                14.000000            79.000000   \n",
       "\n",
       "       Coupling Metric Rules  Coupling Rules  Design Rules  \\\n",
       "count            6091.000000          6091.0   6091.000000   \n",
       "mean                0.236086             0.0      1.913315   \n",
       "std                 0.928698             0.0      6.344052   \n",
       "min                 0.000000             0.0      0.000000   \n",
       "25%                 0.000000             0.0      0.000000   \n",
       "50%                 0.000000             0.0      0.000000   \n",
       "75%                 0.000000             0.0      2.000000   \n",
       "max                16.000000             0.0    157.000000   \n",
       "\n",
       "       Documentation Metric Rules  Empty Code Rules  Finalizer Rules  \\\n",
       "count                 6091.000000       6091.000000      6091.000000   \n",
       "mean                    14.002298          0.023477         0.000985   \n",
       "std                     22.865402          0.215813         0.031373   \n",
       "min                      0.000000          0.000000         0.000000   \n",
       "25%                      5.000000          0.000000         0.000000   \n",
       "50%                      7.000000          0.000000         0.000000   \n",
       "75%                     13.000000          0.000000         0.000000   \n",
       "max                    424.000000          6.000000         1.000000   \n",
       "\n",
       "       Import Statement Rules  Inheritance Metric Rules   J2EE Rules  \\\n",
       "count             6091.000000               6091.000000  6091.000000   \n",
       "mean                 0.063372                  0.097849     0.004433   \n",
       "std                  1.543435                  1.125877     0.075680   \n",
       "min                  0.000000                  0.000000     0.000000   \n",
       "25%                  0.000000                  0.000000     0.000000   \n",
       "50%                  0.000000                  0.000000     0.000000   \n",
       "75%                  0.000000                  0.000000     0.000000   \n",
       "max                 64.000000                 49.000000     2.000000   \n",
       "\n",
       "       JUnit Rules  Jakarta Commons Logging Rules  Java Logging Rules  \\\n",
       "count  6091.000000                    6091.000000         6091.000000   \n",
       "mean      1.301593                       0.057133            0.067477   \n",
       "std      10.250579                       0.535155            0.582907   \n",
       "min       0.000000                       0.000000            0.000000   \n",
       "25%       0.000000                       0.000000            0.000000   \n",
       "50%       0.000000                       0.000000            0.000000   \n",
       "75%       0.000000                       0.000000            0.000000   \n",
       "max     286.000000                      12.000000           17.000000   \n",
       "\n",
       "       JavaBean Rules  MigratingToJUnit4 Rules  Migration Rules  \\\n",
       "count     6091.000000                   6091.0      6091.000000   \n",
       "mean         0.000657                      0.0         0.009030   \n",
       "std          0.025620                      0.0         0.144126   \n",
       "min          0.000000                      0.0         0.000000   \n",
       "25%          0.000000                      0.0         0.000000   \n",
       "50%          0.000000                      0.0         0.000000   \n",
       "75%          0.000000                      0.0         0.000000   \n",
       "max          1.000000                      0.0         4.000000   \n",
       "\n",
       "       Migration13 Rules  Migration14 Rules  Migration15 Rules  Naming Rules  \\\n",
       "count             6091.0             6091.0             6091.0   6091.000000   \n",
       "mean                 0.0                0.0                0.0      0.369726   \n",
       "std                  0.0                0.0                0.0      3.243723   \n",
       "min                  0.0                0.0                0.0      0.000000   \n",
       "25%                  0.0                0.0                0.0      0.000000   \n",
       "50%                  0.0                0.0                0.0      0.000000   \n",
       "75%                  0.0                0.0                0.0      0.000000   \n",
       "max                  0.0                0.0                0.0    133.000000   \n",
       "\n",
       "       Optimization Rules  Security Code Guideline Rules  Size Metric Rules  \\\n",
       "count         6091.000000                    6091.000000        6091.000000   \n",
       "mean             0.219833                       0.019209           2.265474   \n",
       "std              1.156451                       0.202915           9.311114   \n",
       "min              0.000000                       0.000000           0.000000   \n",
       "25%              0.000000                       0.000000           0.000000   \n",
       "50%              0.000000                       0.000000           0.000000   \n",
       "75%              0.000000                       0.000000           2.000000   \n",
       "max             20.000000                       4.000000         364.000000   \n",
       "\n",
       "       Strict Exception Rules  String and StringBuffer Rules  \\\n",
       "count             6091.000000                    6091.000000   \n",
       "mean                 0.088327                       0.143490   \n",
       "std                  0.562437                       1.039814   \n",
       "min                  0.000000                       0.000000   \n",
       "25%                  0.000000                       0.000000   \n",
       "50%                  0.000000                       0.000000   \n",
       "75%                  0.000000                       0.000000   \n",
       "max                 11.000000                      35.000000   \n",
       "\n",
       "       Type Resolution Rules  Unnecessary and Unused Code Rules  \\\n",
       "count            6091.000000                        6091.000000   \n",
       "mean                0.133147                           0.057626   \n",
       "std                 0.743115                           0.437333   \n",
       "min                 0.000000                           0.000000   \n",
       "25%                 0.000000                           0.000000   \n",
       "50%                 0.000000                           0.000000   \n",
       "75%                 0.000000                           0.000000   \n",
       "max                12.000000                          14.000000   \n",
       "\n",
       "       Vulnerability Rules  Number of bugs  \n",
       "count          6091.000000     6091.000000  \n",
       "mean              0.000493        0.003776  \n",
       "std               0.022189        0.061339  \n",
       "min               0.000000        0.000000  \n",
       "25%               0.000000        0.000000  \n",
       "50%               0.000000        0.000000  \n",
       "75%               0.000000        0.000000  \n",
       "max               1.000000        1.000000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد داده‌های آموزش: 4872\n",
      "تعداد داده‌های تست: 1219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"تعداد داده‌های آموزش:\", X_train.shape[0])\n",
    "print(\"تعداد داده‌های تست:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "grouped_data = y_train.groupby(y_train).count()\n",
    "\n",
    "grouped_data.plot(kind='bar')\n",
    "\n",
    "plt.title(\"Distribution of 0s and 1s\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE(sampling_strategy='auto', k_neighbors=2)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "X_test, y_test = oversample.fit_resample(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "grouped_data = y_train.groupby(y_train).count()\n",
    "\n",
    "grouped_data.plot(kind='bar')\n",
    "\n",
    "plt.title(\"Distribution of 0s and 1s\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "def calculate_basic_metrics(y_true, y_pred, Model_name):\n",
    "    global result\n",
    "    metrics = {}\n",
    "    y_pred = (y_pred > 0.5).astype(int) \n",
    "\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred)\n",
    "    metrics['F1-Score'] = f1_score(y_true, y_pred)\n",
    "    metrics['Confusion Matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    result = result.append(pd.Series(metrics, name=Model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradient_boosting_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gradient_boosting_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred_gb, 'Gradient Boosting')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred_xgb, 'XGBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8613\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500077 -> initscore=0.000309\n",
      "[LightGBM] [Info] Start training from score 0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3234, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8659\n",
      "[LightGBM] [Info] Number of data points in the train set: 6469, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499923 -> initscore=-0.000309\n",
      "[LightGBM] [Info] Start training from score -0.000309\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 3235, number of negative: 3235\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8619\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 4852, number of negative: 4852\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9043\n",
      "[LightGBM] [Info] Number of data points in the train set: 9704, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'feature_fraction': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_search = GridSearchCV(lgb_model, param_grid, scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred, 'LightGBM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "9 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Farbod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.95527655        nan 0.95012396 0.88623264 0.97454739        nan\n",
      " 0.94919676 0.89076683 0.98443919        nan 0.9510515  0.8884997 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred, 'Logistic regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "243/243 [==============================] - 2s 4ms/step - loss: 1.1738 - accuracy: 0.8172 - val_loss: 0.3966 - val_accuracy: 0.7702\n",
      "Epoch 2/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.2318 - accuracy: 0.9388 - val_loss: 0.3996 - val_accuracy: 0.7950\n",
      "Epoch 3/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.1128 - accuracy: 0.9726 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0724 - accuracy: 0.9799 - val_loss: 0.0612 - val_accuracy: 0.9954\n",
      "Epoch 5/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0585 - accuracy: 0.9881 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0786 - accuracy: 0.9804 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
      "Epoch 7/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.1301 - accuracy: 0.9750 - val_loss: 0.0485 - val_accuracy: 0.9948\n",
      "Epoch 8/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0277 - accuracy: 0.9943 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0260 - accuracy: 0.9937 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0337 - accuracy: 0.9932 - val_loss: 0.0241 - val_accuracy: 0.9948\n",
      "Epoch 11/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9930 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9955 - val_loss: 5.6920e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0472 - accuracy: 0.9875 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9849 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0761 - accuracy: 0.9863 - val_loss: 0.2233 - val_accuracy: 0.8825\n",
      "Epoch 16/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9898 - val_loss: 0.3067 - val_accuracy: 0.7816\n",
      "Epoch 19/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0169 - val_accuracy: 0.9954\n",
      "Epoch 20/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0730 - accuracy: 0.9885 - val_loss: 0.0410 - val_accuracy: 0.9876\n",
      "Epoch 21/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0381 - accuracy: 0.9897 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0376 - accuracy: 0.9925 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9914 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 0.9959 - val_loss: 6.5253e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.2370 - val_accuracy: 0.8748\n",
      "Epoch 29/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0133 - accuracy: 0.9978 - val_loss: 2.8534e-05 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0554 - accuracy: 0.9884 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
      "Epoch 36/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.9967 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 0.9968 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0356 - accuracy: 0.9897 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0270 - accuracy: 0.9932 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9965 - val_loss: 1.1966e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9976 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9960 - val_loss: 9.0103e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0139 - accuracy: 0.9972 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9992 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0128 - accuracy: 0.9986 - val_loss: 0.0697 - val_accuracy: 0.9717\n",
      "Epoch 51/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 6.4874e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0281 - accuracy: 0.9923 - val_loss: 0.0879 - val_accuracy: 0.9428\n",
      "Epoch 55/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 2.7301e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9961 - val_loss: 7.1158e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 0.9973 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 9.3688e-06 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9941 - val_loss: 5.3862e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9881 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0357 - accuracy: 0.9936 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.0059 - val_accuracy: 0.9954\n",
      "Epoch 81/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 2.2304e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 8.5498e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 4.8971e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.9961 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9991 - val_loss: 5.0750e-04 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 2.1068e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 2.2240e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9943 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0147 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 2.4674e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 6.2381e-05 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9950 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "76/76 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(640, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred, 'Neural Networks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred_svm, 'rbf SVM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_model = SVC(kernel='poly', random_state=42)\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred_svm, 'poly SVM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farbod\\AppData\\Local\\Temp\\ipykernel_5916\\1879831452.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(pd.Series(metrics, name=Model_name))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_model = SVC(kernel='sigmoid', random_state=42)\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "calculate_basic_metrics(y_test, y_pred_svm, 'sigmoid SVM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.499589</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>[[1213, 3], [1214, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.537829</td>\n",
       "      <td>0.081414</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>0.149773</td>\n",
       "      <td>[[1209, 7], [1117, 99]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.502467</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>[[1211, 5], [1205, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.504523</td>\n",
       "      <td>0.012336</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.024291</td>\n",
       "      <td>[[1212, 4], [1201, 15]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.673931</td>\n",
       "      <td>0.369243</td>\n",
       "      <td>0.945263</td>\n",
       "      <td>0.531047</td>\n",
       "      <td>[[1190, 26], [767, 449]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Networks</th>\n",
       "      <td>0.498766</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>[[1210, 6], [1213, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rbf SVM</th>\n",
       "      <td>0.599095</td>\n",
       "      <td>0.330592</td>\n",
       "      <td>0.714032</td>\n",
       "      <td>0.451939</td>\n",
       "      <td>[[1055, 161], [814, 402]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poly SVM</th>\n",
       "      <td>0.494243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[1202, 14], [1216, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigmoid SVM</th>\n",
       "      <td>0.587171</td>\n",
       "      <td>0.599507</td>\n",
       "      <td>0.585072</td>\n",
       "      <td>0.592201</td>\n",
       "      <td>[[699, 517], [487, 729]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy    Recall  Precision  F1-Score  \\\n",
       "Random Forest        0.499589  0.001645   0.400000  0.003276   \n",
       "Gradient Boosting    0.537829  0.081414   0.933962  0.149773   \n",
       "XGBoost              0.502467  0.009046   0.687500  0.017857   \n",
       "LightGBM             0.504523  0.012336   0.789474  0.024291   \n",
       "Logistic regression  0.673931  0.369243   0.945263  0.531047   \n",
       "Neural Networks      0.498766  0.002467   0.333333  0.004898   \n",
       "rbf SVM              0.599095  0.330592   0.714032  0.451939   \n",
       "poly SVM             0.494243  0.000000   0.000000  0.000000   \n",
       "sigmoid SVM          0.587171  0.599507   0.585072  0.592201   \n",
       "\n",
       "                              Confusion Matrix  \n",
       "Random Forest           [[1213, 3], [1214, 2]]  \n",
       "Gradient Boosting      [[1209, 7], [1117, 99]]  \n",
       "XGBoost                [[1211, 5], [1205, 11]]  \n",
       "LightGBM               [[1212, 4], [1201, 15]]  \n",
       "Logistic regression   [[1190, 26], [767, 449]]  \n",
       "Neural Networks         [[1210, 6], [1213, 3]]  \n",
       "rbf SVM              [[1055, 161], [814, 402]]  \n",
       "poly SVM               [[1202, 14], [1216, 0]]  \n",
       "sigmoid SVM           [[699, 517], [487, 729]]  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
